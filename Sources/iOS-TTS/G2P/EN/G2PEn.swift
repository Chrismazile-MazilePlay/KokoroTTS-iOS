import Foundation
import SwiftPOSTagger

/// G2P —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
public class G2PEn: G2P {
    private let isAmericanEnglish: Bool
    private let vocabURL: URL
    private let postagger: SwiftPOSTagger
    private let lexicon: Lexicon
    private let unk: String = "‚ùì"
    
    /// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è G2P –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    /// - Parameters:
    ///   - british: false –¥–ª—è –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, true –¥–ª—è –±—Ä–∏—Ç–∞–Ω—Å–∫–æ–≥–æ
    ///   - vocabURL: URL –ø–∞–ø–∫–∏ —Å vocab —Ñ–∞–π–ª–∞–º–∏ (us_gold.json, us_silver.json, gb_gold.json, gb_silver.json)
    ///   - postaggerModelURL: URL –ø–∞–ø–∫–∏ —Å –º–æ–¥–µ–ª—å—é SwiftPOSTagger (—Å–æ–¥–µ—Ä–∂–∏—Ç Model.mlmodelc, vocab.txt, outTokens.txt)
    public init(british: Bool, vocabURL: URL, postaggerModelURL: URL) throws {
        self.isAmericanEnglish = !british
        self.vocabURL = vocabURL
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º POS tagger
        self.postagger = try SwiftPOSTagger(modelDirectoryURL: postaggerModelURL)
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Lexicon
        self.lexicon = try Lexicon(british: british, vocabURL: vocabURL)
    }
    
    // MARK: - G2P Protocol Implementation
    
    public func convert(_ text: String) throws -> G2PResult {
        
        let preprocessResult = G2PEn.preprocess(text)
        print("üîç G2P.preprocess result: \"\(preprocessResult.result)\"")
        print("üîç G2P.preprocess tokens: \(preprocessResult.tokens)")
        print("üîç G2P.preprocess features: \(preprocessResult.features)")
        
        // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ POS-—Ç–µ–≥–≥–∏–Ω–≥
        var tokens = try tokenize(text: preprocessResult.result, tokens: preprocessResult.tokens, features: preprocessResult.features)
        for (i, token) in tokens.enumerated() {
            print("üîç Token[\(i)]: text='\(token.text)', tag='\(token.tag)', whitespace='\(token.whitespace)', phonemes='\(token.phonemes ?? "nil")', stress=\(token.underscore.stress?.description ?? "nil"), numFlags='\(token.underscore.numFlags)', rating=\(token.underscore.rating?.description ?? "nil"), isHead=\(token.underscore.isHead)")
        }
        // fold_left - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
        tokens = foldLeft(tokens: tokens)
        
        // retokenize - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥—Ç–æ–∫–µ–Ω—ã
        let words = G2PEn.retokenize(tokens: tokens)
        
        // –ü—Ä–æ—Ü–µ—Å—Å –∫–∞–∫ –≤ Python __call__ –º–µ—Ç–æ–¥–∞
        var context = TokenContext()
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –∫–∞–∫ –≤ Python
        for i in stride(from: words.count - 1, through: 0, by: -1) {
            let word = words[i]
            
            if let singleToken = word as? MToken {
                // –û—Ç–¥–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ lexicon
                if singleToken.phonemes == nil {
                    let (phonemes, rating) = lexicon.processToken(singleToken, context: context)
                    singleToken.phonemes = phonemes
                    singleToken.rating = rating
                }
                context = tokenContext(context: context, phonemes: singleToken.phonemes, token: singleToken)
            } else if let tokenGroup = word as? [MToken] {
                // –ì—Ä—É–ø–ø–∞ —Ç–æ–∫–µ–Ω–æ–≤ - —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∫–∞–∫ –≤ Python
                processTokenGroup(tokenGroup, context: &context)
            }
        }
        
        // –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ - —Å–æ–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏ —É–±–∏—Ä–∞–µ–º –≥—Ä—É–ø–ø—ã
        var finalTokens: [MToken] = []
        for word in words {
            if let singleToken = word as? MToken {
                finalTokens.append(singleToken)
            } else if let tokenGroup = word as? [MToken] {
                let mergedToken = mergeTokens(tokenGroup, unk: unk)
                finalTokens.append(mergedToken)
            }
        }
        
        // –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –≤ Python: result = ''.join((self.unk if tk.phonemes is None else tk.phonemes) + tk.whitespace for tk in tokens)
        let phonemeString = finalTokens.map { token in
            let phonemes = token.phonemes ?? unk
            return phonemes + token.whitespace
        }.joined()
        
        print("üîç Final phoneme string: \"\(phonemeString)\"")
        
        return G2PResult(phonemeString: phonemeString, tokens: finalTokens)
    }
    
    /// –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python G2P.tokenize
    public func tokenize(text: String, tokens: [String], features: [Int: Any]) throws -> [MToken] {
        
        // doc = self.nlp(text) - –∏—Å–ø–æ–ª—å–∑—É–µ–º SwiftPOSTagger
        let tokenTagPairs = try postagger.predict(text: text)
        
        // –°–æ–∑–¥–∞–µ–º MToken –æ–±—ä–µ–∫—Ç—ã –∫–∞–∫ –≤ Python
        var mutableTokens: [MToken] = []
        for (index, (tokenText, tag)) in tokenTagPairs.enumerated() {
            // –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –Ω–µ –∏–º–µ–µ—Ç whitespace, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏–º–µ—é—Ç –ø—Ä–æ–±–µ–ª
            let whitespace = (index == tokenTagPairs.count - 1) ? "" : " "
            let mtoken = MToken(
                text: tokenText,
                tag: tag,
                whitespace: whitespace,
                underscore: MToken.Underscore(isHead: true, numFlags: "", prespace: false)
            )
            mutableTokens.append(mtoken)
        }
        
        // if not features: return mutable_tokens
        if features.isEmpty {
            let correctedTokens = fixApostropheTokens(mutableTokens)
            logTokenizeResult(correctedTokens)
            return correctedTokens
        }
        
        // –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏—á–∏ –∫ —Ç–æ–∫–µ–Ω–∞–º (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ spacy alignment)
        // –í Python –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è spacy.training.Alignment.from_strings –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        // –ú—ã –¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
        for (featureIndex, featureValue) in features {
            if featureIndex < mutableTokens.count {
                let token = mutableTokens[featureIndex]
                
                // assert isinstance(v, str) or isinstance(v, int) or v in (0.5, -0.5)
                if let intValue = featureValue as? Int {
                    // if not isinstance(v, str): mutable_tokens[j]._.stress = v
                    token.underscore.stress = Double(intValue)
                } else if let doubleValue = featureValue as? Double {
                    token.underscore.stress = doubleValue
                } else if let stringValue = featureValue as? String {
                    if stringValue.hasPrefix("/") {
                        // elif v.startswith('/'): 
                        //   mutable_tokens[j]._.is_head = i == 0
                        //   mutable_tokens[j].phonemes = v.lstrip('/') if i == 0 else ''
                        //   mutable_tokens[j]._.rating = 5
                        token.underscore.isHead = true  // i == 0 (first occurrence)
                        token.phonemes = String(stringValue.dropFirst()) // v.lstrip('/')
                        token.underscore.rating = 5
                    } else if stringValue.hasPrefix("#") {
                        // elif v.startswith('#'): mutable_tokens[j]._.num_flags = v.lstrip('#')
                        token.underscore.numFlags = String(stringValue.dropFirst()) // v.lstrip('#')
                    }
                }
            }
        }
        
        // Post-process –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–ø–æ—Å—Ç—Ä–æ—Ñ–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∫ –≤ Python spaCy
        let correctedTokens = fixApostropheTokens(mutableTokens)
        logTokenizeResult(correctedTokens)
        
        return correctedTokens
    }
    
    /// –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Python spaCy
    private func fixApostropheTokens(_ tokens: [MToken]) -> [MToken] {
        var result: [MToken] = []
        var i = 0
        
        while i < tokens.count {
            let token = tokens[i]
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (—É—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∏–¥—ã –∞–ø–æ—Å—Ç—Ä–æ—Ñ–æ–≤)
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω - —ç—Ç–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ (–ª—é–±–æ–π –≤–∏–¥)
            if i + 1 < tokens.count {
                let nextTokenText = tokens[i + 1].text
                print("üîç Checking token pair: '\(token.text)' + '\(nextTokenText)' (length: \(nextTokenText.count), unicode: \(nextTokenText.unicodeScalars.map { String($0.value, radix: 16) }.joined()))")
            }
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –ø–æ—Ö–æ–∂ –Ω–∞ –∞–ø–æ—Å—Ç—Ä–æ—Ñ
            var isApostrophe = false
            if i + 1 < tokens.count {
                let nextText = tokens[i + 1].text
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ Unicode scalar value
                if nextText.count == 1, let scalar = nextText.unicodeScalars.first {
                    // U+0027 ('), U+2019 ('), U+2018 (')
                    isApostrophe = (scalar.value == 0x27 || scalar.value == 0x2019 || scalar.value == 0x2018)
                }
            }
            
            print("üîç isApostrophe = \(isApostrophe) for token '\(i + 1 < tokens.count ? tokens[i + 1].text : "N/A")'")
            
            if i + 2 < tokens.count && isApostrophe {
                print("üîç Found apostrophe pattern: '\(token.text)' + '\(tokens[i + 1].text)' + '\(tokens[i + 2].text)'")
                let nextToken = tokens[i + 2]
                let combined = token.text + "'" + nextToken.text
                
                // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—â–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫–∞–∫ –≤ spaCy
                if shouldCombineApostrophe(word: token.text, suffix: nextToken.text) {
                    let newText = getSpacyTokenization(for: combined)
                    
                    if newText.count == 2 {
                        // –°–æ–∑–¥–∞–µ–º –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –∫–∞–∫ –≤ spaCy (–Ω–∞–ø—Ä–∏–º–µ—Ä, "can't" ‚Üí ["ca", "n't"])
                        let firstToken = MToken(
                            text: newText[0],
                            tag: token.tag,
                            whitespace: "",
                            underscore: token.underscore
                        )
                        
                        let secondToken = MToken(
                            text: newText[1], 
                            tag: nextToken.tag,
                            whitespace: nextToken.whitespace,
                            underscore: nextToken.underscore
                        )
                        
                        result.append(firstToken)
                        result.append(secondToken)
                        
                        i += 3 // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ 2 —Ç–æ–∫–µ–Ω–∞
                        continue
                    }
                }
            }
            
            // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –µ—Å—Ç—å
            result.append(token)
            i += 1
        }
        
        return result
    }
    
    /// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –∞–ø–æ—Å—Ç—Ä–æ—Ñ
    private func shouldCombineApostrophe(word: String, suffix: String) -> Bool {
        let commonContractions = [
            "s", "t", "d", "m", "re", "ve", "ll"  // 's, 't, 'd, 'm, 're, 've, 'll
        ]
        return commonContractions.contains(suffix.lowercased())
    }
    
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∫–∞–∫ –≤ Python spaCy
    private func getSpacyTokenization(for word: String) -> [String] {
        let lowered = word.lowercased()
        
        // –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –∫–∞–∫ –≤ spaCy
        switch lowered {
        case "can't":
            return ["ca", "n't"]
        case "won't":
            return ["wo", "n't"]
        case "shan't":
            return ["sha", "n't"]
        case "wouldn't", "couldn't", "shouldn't", "hadn't", "haven't", "hasn't", "wasn't", "weren't", "isn't", "aren't", "don't", "doesn't", "didn't":
            if let apostropheIndex = lowered.firstIndex(of: "'") {
                let prefix = String(lowered[..<apostropheIndex])
                let suffix = String(lowered[apostropheIndex...])
                return [prefix, suffix]
            }
        default:
            // –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ ('s, 'd, 'm, 're, 've, 'll)
            if let apostropheIndex = lowered.firstIndex(of: "'") {
                let prefix = String(lowered[..<apostropheIndex])
                let suffix = String(lowered[apostropheIndex...])
                return [prefix, suffix]
            }
        }
        
        return [word] // Fallback
    }
    
    /// –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    private func logTokenizeResult(_ tokens: [MToken]) {
        print("üîç Tokenize result (\(tokens.count) tokens):")
        for (i, token) in tokens.enumerated() {
            print("  [\(i)]: '\(token.text)' (tag: \(token.tag))")
        }
    }
    
    /// fold_left –º–µ—Ç–æ–¥ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python G2P.fold_left
    public func foldLeft(tokens: [MToken]) -> [MToken] {
        var result: [MToken] = []
        for tk in tokens {
            let processedToken: MToken
            
            // tk = merge_tokens([result.pop(), tk], unk=self.unk) if result and not tk._.is_head else tk
            if !result.isEmpty && !tk.underscore.isHead {
                let lastToken = result.removeLast() // result.pop()
                processedToken = mergeTokens([lastToken, tk], unk: "‚ùì") // self.unk = "‚ùì"
            } else {
                processedToken = tk
            }
            
            result.append(processedToken)
        }
        
        return result
    }
    
    /// merge_tokens —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python
    private func mergeTokens(_ tokens: [MToken], unk: String?) -> MToken {
        // stress = {tk._.stress for tk in tokens if tk._.stress is not None}
        let stressValues = Set(tokens.compactMap { $0.underscore.stress })
        
        // currency = {tk._.currency for tk in tokens if tk._.currency is not None}
        let currencyValues = Set(tokens.compactMap { $0.underscore.currency })
        
        // rating = {tk._.rating for tk in tokens}
        let ratingValues = tokens.map { $0.underscore.rating }
        
        // –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ–Ω–µ–º
        let phonemes: String?
        if unk == nil {
            phonemes = nil
        } else {
            var phonemesStr = ""
            for tk in tokens {
                // if tk._.prespace and phonemes and not phonemes[-1].isspace() and tk.phonemes:
                if tk.underscore.prespace && !phonemesStr.isEmpty && !phonemesStr.last!.isWhitespace && tk.phonemes != nil {
                    phonemesStr += " "
                }
                // phonemes += unk if tk.phonemes is None else tk.phonemes
                phonemesStr += tk.phonemes ?? unk!
            }
            phonemes = phonemesStr
        }
        
        // text=''.join(tk.text + tk.whitespace for tk in tokens[:-1]) + tokens[-1].text
        var text = ""
        for (index, tk) in tokens.enumerated() {
            if index < tokens.count - 1 {
                text += tk.text + tk.whitespace
            } else {
                text += tk.text
            }
        }
        
        // tag=max(tokens, key=lambda tk: sum(1 if c == c.lower() else 2 for c in tk.text)).tag
        let selectedToken = tokens.max { tk1, tk2 in
            let score1 = tk1.text.reduce(0) { sum, c in sum + (c.isLowercase ? 1 : 2) }
            let score2 = tk2.text.reduce(0) { sum, c in sum + (c.isLowercase ? 1 : 2) }
            return score1 < score2
        }!
        
        let mergedUnderscore = MToken.Underscore(
            isHead: tokens[0].underscore.isHead,                               // is_head=tokens[0]._.is_head
            alias: nil,                                                        // alias=None
            stress: stressValues.count == 1 ? stressValues.first : nil,       // stress=list(stress)[0] if len(stress) == 1 else None
            currency: currencyValues.max(),                                    // currency=max(currency) if currency else None
            numFlags: String(Set(tokens.flatMap { $0.underscore.numFlags }).sorted()), // num_flags=''.join(sorted({c for tk in tokens for c in tk._.num_flags}))
            prespace: tokens[0].underscore.prespace,                          // prespace=tokens[0]._.prespace
            rating: ratingValues.contains(nil) ? nil : ratingValues.compactMap { $0 }.min() // rating=None if None in rating else min(rating)
        )
        
        return MToken(
            text: text,
            tag: selectedToken.tag,
            whitespace: tokens.last!.whitespace,           // whitespace=tokens[-1].whitespace
            phonemes: phonemes,
            startTs: tokens.first!.startTs,               // start_ts=tokens[0].start_ts
            endTs: tokens.last!.endTs,                    // end_ts=tokens[-1].end_ts
            underscore: mergedUnderscore
        )
    }
    
    /// retokenize –º–µ—Ç–æ–¥ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python G2P.retokenize
    public static func retokenize(tokens: [MToken]) -> [Any] {
        print("üîç G2P.retokenize input: \(tokens.count) tokens")
        
        var words: [Any] = []
        var currency: String? = nil
        
        for (i, token) in tokens.enumerated() {
            let tks: [MToken]
            
            // if token._.alias is None and token.phonemes is None:
            if token.underscore.alias == nil && token.phonemes == nil {
                // tks = [replace(token, text=t, whitespace='', _=MToken.Underscore(...)) for t in subtokenize(token.text)]
                let subtokens = subtokenize(token.text)
                tks = subtokens.map { subtext in
                    let newToken = MToken(
                        text: subtext,
                        tag: token.tag,
                        whitespace: "", // –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—É—Å—Ç–æ–π
                        underscore: MToken.Underscore(
                            isHead: true,
                            stress: token.underscore.stress,
                            numFlags: token.underscore.numFlags,
                            prespace: false
                        )
                    )
                    return newToken
                }
                print("üîç G2P.retokenize subtokenized '\(token.text)' into: \(subtokens)")
            } else {
                tks = [token]
            }
            
            // tks[-1].whitespace = token.whitespace
            tks.last?.whitespace = token.whitespace
            print("üîç G2P.retokenize processing subtokens: \(tks.map { "'\($0.text)'(ws:'\($0.whitespace)')" })")
            
            for (j, tk) in tks.enumerated() {
                // if tk._.alias is not None or tk.phonemes is not None: pass
                if tk.underscore.alias != nil || tk.phonemes != nil {
                    // pass - –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
                }
                // elif tk.tag == '$' and tk.text in CURRENCIES:
                else if tk.tag == "$" && ["$", "¬£", "‚Ç¨"].contains(tk.text) {
                    currency = tk.text
                    tk.phonemes = ""
                    tk.underscore.rating = 4
                    print("üîç G2P.retokenize found currency: '\(tk.text)'")
                }
                // elif tk.tag == ':' and tk.text in ('-', '‚Äì'):
                else if tk.tag == ":" && ["-", "‚Äì"].contains(tk.text) {
                    tk.phonemes = "‚Äî"
                    tk.underscore.rating = 3
                    print("üîç G2P.retokenize converted dash: '\(tk.text)' -> '‚Äî'")
                }
                // elif tk.tag in PUNCT_TAGS and not all(97 <= ord(c.lower()) <= 122 for c in tk.text):
                else if isPunctTag(tk.tag) && !tk.text.allSatisfy({ c in c.isLetter }) {
                    let punctMap = ["-LRB-": "(", "-RRB-": ")", "``": "\u{201C}", "\"\"": "\u{201D}", "''": "\u{201D}"]
                    tk.phonemes = punctMap[tk.tag] ?? tk.text.filter { ";:,.!?‚Äî‚Ä¶\"".contains($0) }.map(String.init).joined()
                    tk.underscore.rating = 4
                    print("üîç G2P.retokenize handled punctuation: '\(tk.text)' -> '\(tk.phonemes ?? "")'")
                }
                // elif currency is not None:
                else if currency != nil {
                    if tk.tag != "CD" {
                        currency = nil
                    } else if j + 1 == tks.count && (i + 1 == tokens.count || tokens[i + 1].tag != "CD") {
                        tk.underscore.currency = currency
                        print("üîç G2P.retokenize assigned currency '\(currency!)' to '\(tk.text)'")
                    }
                }
                // elif 0 < j < len(tks)-1 and tk.text == '2' and (tks[j-1].text[-1]+tks[j+1].text[0]).isalpha():
                else if j > 0 && j < tks.count - 1 && tk.text == "2" {
                    let prevLast = tks[j-1].text.last
                    let nextFirst = tks[j+1].text.first
                    if let prev = prevLast, let next = nextFirst, prev.isLetter && next.isLetter {
                        tk.underscore.alias = "to"
                        print("üîç G2P.retokenize converted '2' to 'to' between letters")
                    }
                }
                
                // –õ–æ–≥–∏–∫–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
                // if tk._.alias is not None or tk.phonemes is not None: words.append(tk)
                if tk.underscore.alias != nil || tk.phonemes != nil {
                    words.append(tk)
                    print("üîç G2P.retokenize added token with alias/phonemes: '\(tk.text)'")
                }
                // elif words and isinstance(words[-1], list) and not words[-1][-1].whitespace:
                else if !words.isEmpty,
                        let lastWordArray = words.last as? [MToken],
                        lastWordArray.last?.whitespace.isEmpty == true {
                    // –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≥—Ä—É–ø–ø–µ
                    tk.underscore.isHead = false
                    var updatedArray = lastWordArray
                    updatedArray.append(tk)
                    words[words.count - 1] = updatedArray
                    print("üîç G2P.retokenize added '\(tk.text)' to existing group (isHead=false)")
                }
                // else: words.append(tk if tk.whitespace else [tk])
                else {
                    print("üîç G2P.retokenize token '\(tk.text)' has whitespace: '\(tk.whitespace)' (isEmpty: \(tk.whitespace.isEmpty))")
                    if !tk.whitespace.isEmpty {
                        // –¢–æ–∫–µ–Ω —Å whitespace - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π
                        words.append(tk)
                        print("üîç G2P.retokenize added single token '\(tk.text)' (has whitespace)")
                    } else {
                        // –¢–æ–∫–µ–Ω –±–µ–∑ whitespace - —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥—Ä—É–ø–ø–µ
                        print("üîç G2P.retokenize current words count: \(words.count), last is array: \(words.last is [MToken])")
                        if !words.isEmpty, let lastWordArray = words.last as? [MToken] {
                            // –ü–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç - –≥—Ä—É–ø–ø–∞, –¥–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–µ–π
                            tk.underscore.isHead = false
                            var updatedArray = lastWordArray
                            updatedArray.append(tk)
                            words[words.count - 1] = updatedArray
                            print("üîç G2P.retokenize added '\(tk.text)' to existing group (isHead=false)")
                        } else {
                            // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
                            words.append([tk])
                            print("üîç G2P.retokenize created new group with '\(tk.text)'")
                        }
                    }
                }
            }
        }
        
        // return [w[0] if isinstance(w, list) and len(w) == 1 else w for w in words]
        let result = words.map { word -> Any in
            if let tokenArray = word as? [MToken], tokenArray.count == 1 {
                return tokenArray[0]
            }
            return word
        }
        
        print("üîç G2P.retokenize output: \(result.count) words")
        
        return result
    }
    
    /// –§—É–Ω–∫—Ü–∏—è subtokenize - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Python regex
    private static func subtokenize(_ word: String) -> [String] {
        // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ Python regex
        // –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        var result: [String] = []
        var current = ""
        
        for char in word {
            if char.isLetter || char.isNumber {
                current += String(char)
            } else {
                if !current.isEmpty {
                    result.append(current)
                    current = ""
                }
                if !char.isWhitespace {
                    result.append(String(char))
                }
            }
        }
        
        if !current.isEmpty {
            result.append(current)
        }
        
        return result.isEmpty ? [word] : result
    }
    
    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–≥ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–º
    private static func isPunctTag(_ tag: String) -> Bool {
        let punctTags = [".", ",", "-LRB-", "-RRB-", "``", "\"\"", "''", ":", "$", "#", "NFP"]
        return punctTags.contains(tag)
    }
    
    
    // MARK: - Token processing helpers
    
    /// –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–æ–∫–µ–Ω–∞ (–∫–∞–∫ token_context –≤ Python)
    private func tokenContext(context: TokenContext, phonemes: String?, token: MToken) -> TokenContext {
        var vowel = context.futureVowel
        
        if let phonemes = phonemes {
            // vowel = next((None if c in NON_QUOTE_PUNCTS else (c in VOWELS) for c in ps if any(c in s for s in (VOWELS, CONSONANTS, NON_QUOTE_PUNCTS))), vowel)
            let vowelChars = Set("AIOQWYaiu√¶…ë…í…î…ô…õ…ú…™ ä å·µª")
            let consonantChars = Set("bdfhjklmnpstvwz√∞≈ã…°…π…æ É í § ßŒ∏")
            let nonQuotePuncts = Set(";:,.!?‚Äî‚Ä¶")
            
            for char in phonemes {
                _ = String(char)
                if vowelChars.contains(char) || consonantChars.contains(char) || nonQuotePuncts.contains(char) {
                    if nonQuotePuncts.contains(char) {
                        vowel = nil
                    } else {
                        vowel = vowelChars.contains(char)
                    }
                    break
                }
            }
        }
        
        // future_to = token.text in ('to', 'To') or (token.text == 'TO' and token.tag in ('TO', 'IN'))
        let futureTo = token.text == "to" || token.text == "To" || (token.text == "TO" && (token.tag == "TO" || token.tag == "IN"))
        
        return TokenContext(futureVowel: vowel, futureTo: futureTo)
    }
    
    /// –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ Python)
    private func processTokenGroup(_ tokenGroup: [MToken], context: inout TokenContext) {
        var left = 0
        var right = tokenGroup.count
        var shouldFallback = false
        
        while left < right {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ñ–æ–Ω–µ–º—ã –∏–ª–∏ –∞–ª–∏–∞—Å—ã
            let hasPhonemes = tokenGroup[left..<right].contains { $0.underscore.alias != nil || $0.phonemes != nil }
            
            let mergedToken: MToken?
            if hasPhonemes {
                mergedToken = nil
            } else {
                mergedToken = mergeTokens(Array(tokenGroup[left..<right]), unk: nil)
            }
            
            let (phonemes, rating): (String?, Int?)
            if let mergedToken = mergedToken {
                (phonemes, rating) = lexicon.processToken(mergedToken, context: context)
            } else {
                (phonemes, rating) = (nil, nil)
            }
            
            if phonemes != nil {
                // –£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω—ã —Ñ–æ–Ω–µ–º—ã –¥–ª—è –≥—Ä—É–ø–ø—ã
                tokenGroup[left].phonemes = phonemes
                tokenGroup[left].rating = rating
                
                // –û—á–∏—â–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –≥—Ä—É–ø–ø–µ
                for i in (left + 1)..<right {
                    tokenGroup[i].phonemes = ""
                    tokenGroup[i].rating = rating
                }
                
                context = tokenContext(context: context, phonemes: phonemes, token: mergedToken!)
                right = left
                left = 0
            } else if left + 1 < right {
                left += 1
            } else {
                // –ù–µ –º–æ–∂–µ–º –Ω–∞–π—Ç–∏ —Ñ–æ–Ω–µ–º—ã, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
                right -= 1
                let currentToken = tokenGroup[right]
                
                if currentToken.phonemes == nil {
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ junk —Å–∏–º–≤–æ–ª—ã
                    let subtokenJunks = Set("',-._''/")
                    if currentToken.text.allSatisfy({ subtokenJunks.contains($0) }) {
                        currentToken.phonemes = ""
                        currentToken.underscore.rating = 3
                    } else {
                        // –ù—É–∂–µ–Ω fallback, –Ω–æ —É –Ω–∞—Å –µ–≥–æ –Ω–µ—Ç - –ø—Ä–æ—Å—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ–º nil
                        shouldFallback = true
                        break
                    }
                }
                left = 0
            }
        }
        
        if shouldFallback {
            // –í Python –∑–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback, –Ω–æ —É –Ω–∞—Å –µ–≥–æ –Ω–µ—Ç
            // –ü—Ä–æ—Å—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            _ = mergeTokens(tokenGroup, unk: nil)
            tokenGroup[0].phonemes = nil  // –û—Å—Ç–∞–≤–ª—è–µ–º nil, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ unk
            tokenGroup[0].underscore.rating = 1
            for i in 1..<tokenGroup.count {
                tokenGroup[i].phonemes = ""
                tokenGroup[i].rating = 1
            }
        } else {
            // Resolve tokens logic –∏–∑ Python
            resolveTokens(tokenGroup)
        }
    }
    
    /// –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (–∫–∞–∫ resolve_tokens –≤ Python)
    private func resolveTokens(_ tokens: [MToken]) {
        // text = ''.join(tk.text + tk.whitespace for tk in tokens[:-1]) + tokens[-1].text
        var text = ""
        for (i, token) in tokens.enumerated() {
            if i < tokens.count - 1 {
                text += token.text + token.whitespace
            } else {
                text += token.text
            }
        }
        
        // prespace = ' ' in text or '/' in text or len({0 if c.isalpha() else (1 if is_digit(c) else 2) for c in text if c not in SUBTOKEN_JUNKS}) > 1
        let subtokenJunks = Set("',-._''/")
        let categories = Set(text.compactMap { char -> Int? in
            guard !subtokenJunks.contains(char) else { return nil }
            if char.isLetter { return 0 }
            else if char.isNumber { return 1 }
            else { return 2 }
        })
        let prespace = text.contains(" ") || text.contains("/") || categories.count > 1
        
        // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º prespace –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞—á–∏–Ω–∞—è —Å–æ –≤—Ç–æ—Ä–æ–≥–æ
        for i in 1..<tokens.count {
            if tokens[i].phonemes != nil {
                tokens[i].underscore.prespace = prespace
            }
        }
        
        if prespace { return }
        
        // –õ–æ–≥–∏–∫–∞ —É–¥–∞—Ä–µ–Ω–∏–π –∏–∑ Python (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        let primaryStress = "Àà"
        let stressWeight = { (phonemes: String?) -> Int in
            guard let phonemes = phonemes else { return 0 }
            let diphthongs = Set("AIOQWY § ß")
            return phonemes.reduce(0) { sum, char in
                sum + (diphthongs.contains(char) ? 2 : 1)
            }
        }
        
        let indices: [(Bool, Int, Int)] = tokens.enumerated().compactMap { (i, token) in
            guard let phonemes = token.phonemes, !phonemes.isEmpty else { return nil }
            return (phonemes.contains(primaryStress), stressWeight(phonemes), i)
        }
        
        if indices.count == 2 && tokens[indices[0].2].text.count == 1 {
            let i = indices[1].2
            if let phonemes = tokens[i].phonemes {
                tokens[i].phonemes = applyStress(phonemes, stress: -0.5)
            }
            return
        } else if indices.count < 2 || indices.filter({ $0.0 }).count <= (indices.count + 1) / 2 {
            return
        }
        
        let sortedIndices = indices.sorted { $0.1 < $1.1 }
        let toReduce = Array(sortedIndices.prefix(indices.count / 2))
        
        for (_, _, i) in toReduce {
            if let phonemes = tokens[i].phonemes {
                tokens[i].phonemes = applyStress(phonemes, stress: -0.5)
            }
        }
    }
    
    /// –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–¥–∞—Ä–µ–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑ Lexicon)
    private func applyStress(_ phonemes: String, stress: Double) -> String {
        // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω—è–µ–º —É–¥–∞—Ä–µ–Ω–∏—è
        let primaryStress = "Àà"
        let secondaryStress = "Àå"
        
        if stress == -0.5 {
            return phonemes.replacingOccurrences(of: primaryStress, with: secondaryStress)
        }
        
        return phonemes
    }
    
    /// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–∞–∫ –≤ Python: result, tokens, features)
    public struct PreprocessResult {
        let result: String
        let tokens: [String] 
        let features: [Int: Any]
    }
    
    /// –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python G2P.preprocess
    public static func preprocess(_ text: String) -> PreprocessResult {
        print("üîç G2P.preprocess input: \"\(text)\"")
        
        var result = ""
        var tokens: [String] = []
        var features: [Int: Any] = [:]
        var lastEnd = 0
        
        // text = text.lstrip() - —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã —Å–ª–µ–≤–∞
        let trimmedText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        
        // Regex –¥–ª—è markdown —Å—Å—ã–ª–æ–∫: \[([^\]]+)\]\(([^\)]*)\)
        let linkRegex = try! NSRegularExpression(pattern: #"\[([^\]]+)\]\(([^\)]*)\)"#)
        let matches = linkRegex.matches(in: trimmedText, range: NSRange(trimmedText.startIndex..., in: trimmedText))
        
        for match in matches {
            // result += text[last_end:m.start()]
            let beforeRange = NSRange(location: lastEnd, length: match.range.location - lastEnd)
            if let beforeSwiftRange = Range(beforeRange, in: trimmedText) {
                let beforeText = String(trimmedText[beforeSwiftRange])
                result += beforeText
                // tokens.extend(text[last_end:m.start()].split())
                tokens.append(contentsOf: beforeText.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
            }
            
            if let textRange = Range(match.range(at: 1), in: trimmedText),
               let featureRange = Range(match.range(at: 2), in: trimmedText) {
                
                let linkText = String(trimmedText[textRange])      // m.group(1)
                let featureString = String(trimmedText[featureRange]) // m.group(2)
                
                // –ü–∞—Ä—Å–∏–º —Ñ–∏—á—É —Å–æ–≥–ª–∞—Å–Ω–æ Python –ª–æ–≥–∏–∫–µ
                let f = parseFeature(featureString)
                if f != nil {
                    features[tokens.count] = f
                }
                
                result += linkText
                tokens.append(linkText)
                lastEnd = match.range.location + match.range.length
            }
        }
        
        // if last_end < len(text): result += text[last_end:]; tokens.extend(text[last_end:].split())
        if lastEnd < trimmedText.count {
            let remainingRange = NSRange(location: lastEnd, length: trimmedText.count - lastEnd)
            if let remainingSwiftRange = Range(remainingRange, in: trimmedText) {
                let remainingText = String(trimmedText[remainingSwiftRange])
                result += remainingText
                tokens.append(contentsOf: remainingText.components(separatedBy: .whitespacesAndNewlines).filter { !$0.isEmpty })
            }
        }
        
        let preprocessResult = PreprocessResult(result: result, tokens: tokens, features: features)
        
        return preprocessResult
    }
    
    /// –ü–∞—Ä—Å–∏—Ç —Ñ–∏—á–∏ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Python
    private static func parseFeature(_ featureString: String) -> Any? {
        var f = featureString
        
        // if is_digit(f[1 if f[:1] in ('-', '+') else 0:]):
        let startIndex = (f.hasPrefix("-") || f.hasPrefix("+")) ? 1 : 0
        let numberPart = String(f.dropFirst(startIndex))
        if numberPart.allSatisfy({ $0.isNumber }) {
            return Int(f) ?? nil
        }
        
        // elif f in ('0.5', '+0.5'): f = 0.5
        if f == "0.5" || f == "+0.5" {
            return 0.5
        }
        // elif f == '-0.5': f = -0.5  
        else if f == "-0.5" {
            return -0.5
        }
        // elif len(f) > 1 and f[0] == '/' and f[-1] == '/': f = f[0] + f[1:].rstrip('/')
        else if f.count > 1 && f.hasPrefix("/") && f.hasSuffix("/") {
            f = "/" + String(f.dropFirst().trimmingCharacters(in: CharacterSet(charactersIn: "/")))
            return f
        }
        // elif len(f) > 1 and f[0] == '#' and f[-1] == '#': f = f[0] + f[1:].rstrip('#')
        else if f.count > 1 && f.hasPrefix("#") && f.hasSuffix("#") {
            f = "#" + String(f.dropFirst().trimmingCharacters(in: CharacterSet(charactersIn: "#")))
            return f
        }
        // else: f = None
        else {
            return nil
        }
    }
}
